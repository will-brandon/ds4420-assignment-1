{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98645648-2d3e-4e74-b6ca-2f71903f0ef1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d983657a-6616-4671-80b4-e8dd95c3050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Optional, Any, Tuple, List\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644d15c5-21b6-4bb5-beeb-053ab71f65d7",
   "metadata": {},
   "source": [
    "## Reading Datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9eb5734-06a1-44c4-b71c-ccb525e95205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(filename: str) -> pd.DataFrame:\n",
    "    return pd.read_csv(filename, sep=',', encoding='latin-1')\n",
    "\n",
    "train_and_val_data = read_csv('data/ds4420_kaggle_train_data.csv')\n",
    "test_data = read_csv('data/ds4420_kaggle_test_data.csv')\n",
    "\n",
    "def split_label(data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    return data.drop('Label', axis=1), data['Label']\n",
    "\n",
    "X_train_and_val, y_train_and_val = split_label(train_and_val_data)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_and_val,\n",
    "                                                      y_train_and_val,\n",
    "                                                      test_size=0.2,\n",
    "                                                      random_state=1)\n",
    "\n",
    "X_test = test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eff251-788e-4728-ba1c-896ac5ea7beb",
   "metadata": {},
   "source": [
    "## Custom Feature Extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b6fe854-7992-415c-927d-946e2fd05213",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThresholdClassifier(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    model: BaseEstimator\n",
    "    threshold: int\n",
    "\n",
    "    def __init__(self, model: BaseEstimator, threshold: int=0.5) -> None:\n",
    "        BaseEstimator.__init__(self)\n",
    "        TransformerMixin.__init__(self)\n",
    "        self.model = model\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X: pd.Series, y: Optional[Any]=None) -> ThresholdClassifier:\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: pd.Series) -> pd.DataFrame:\n",
    "        return self.model.predict_proba(X)[:,1] > self.threshold\n",
    "\n",
    "class InteractFeaturesTransformer(FunctionTransformer):\n",
    "    \"\"\"\n",
    "    Multiplies two features together to yield a new feature.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        FunctionTransformer.__init__(self, self.__interact_features, validate=True)\n",
    "\n",
    "    def __interact_features(self, X: pd.DataFrame) -> pd.Series:\n",
    "        return X[:, 0:1] * X[:, 1:2]\n",
    "\n",
    "class LengthFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Converts a text feature into its length.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        BaseEstimator.__init__(self)\n",
    "        TransformerMixin.__init__(self)\n",
    "    \n",
    "    def fit(self, X: pd.Series, y: Optional[Any]=None) -> LengthFeatureExtractor:\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X: pd.Series) -> pd.DataFrame:\n",
    "        yelling_feature = X.apply(lambda text: len(text))\n",
    "        return yelling_feature.values.reshape(-1, 1)\n",
    "\n",
    "class CapsBinaryFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Converts a text feature into a binary feature where 1 denotes that at least one word in the text\n",
    "    is completely capitalized, which is assumed to be yelling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        BaseEstimator.__init__(self)\n",
    "        TransformerMixin.__init__(self)\n",
    "        \n",
    "    def __text_contains_caps_binary(self, text: str) -> int:\n",
    "        return int(any(word.isupper() for word in text.split(' ')))\n",
    "    \n",
    "    def fit(self, X: pd.Series, y: Optional[Any]=None) -> CapsBinaryFeatureExtractor:\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X: pd.Series) -> pd.DataFrame:\n",
    "        yelling_feature = X.apply(self.__text_contains_caps_binary)\n",
    "        return yelling_feature.values.reshape(-1, 1)\n",
    "\n",
    "class CapsCountFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Converts a text feature into a numeric feature count of the number of words in the text that are\n",
    "    completely capitalized, which is assumed to be yelling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        BaseEstimator.__init__(self)\n",
    "        TransformerMixin.__init__(self)\n",
    "    \n",
    "    def __text_caps_count(self, text: str) -> int:\n",
    "        return int(sum(1 for word in text.split(' ') if word.isupper()))\n",
    "    \n",
    "    def fit(self, X: pd.Series, y: Optional[Any]=None) -> CapsCountFeatureExtractor:\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.Series) -> pd.DataFrame:\n",
    "        yelling_feature = X.apply(self.__text_caps_count)\n",
    "        return yelling_feature.values.reshape(-1, 1)\n",
    "\n",
    "class CharBinaryFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Converts a text feature into binary feature where 1 indicates the text containst at least one of\n",
    "    the given character.\n",
    "    \"\"\"\n",
    "\n",
    "    char: str\n",
    "    \n",
    "    def __init__(self, char: str) -> None:\n",
    "        BaseEstimator.__init__(self)\n",
    "        TransformerMixin.__init__(self)\n",
    "\n",
    "        if len(char) != 1:\n",
    "            raise ValueError('Character must be a string of length 1.')\n",
    "        \n",
    "        self.char = char\n",
    "    \n",
    "    def fit(self, X: pd.Series, y: Optional[Any]=None) -> ExclamationBinaryFeatureExtractor:\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.Series) -> pd.DataFrame:\n",
    "        yelling_feature = X.apply(lambda text: 0 + (self.char in text))\n",
    "        return yelling_feature.values.reshape(-1, 1)\n",
    "\n",
    "class CharCountFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Converts a text feature into a numeric feature count of the number of exclamation marks, which is\n",
    "    assumed to be yelling.\n",
    "    \"\"\"\n",
    "\n",
    "    char: str\n",
    "    \n",
    "    def __init__(self, char: str) -> None:\n",
    "        BaseEstimator.__init__(self)\n",
    "        TransformerMixin.__init__(self)\n",
    "    \n",
    "        if len(char) != 1:\n",
    "            raise ValueError('Character must be a string of length 1.')\n",
    "        \n",
    "        self.char = char\n",
    "\n",
    "    def fit(self, X: pd.Series, y: Optional[Any]=None) -> ExclamationCountFeatureExtractor:\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.Series) -> pd.DataFrame:\n",
    "        yelling_feature = X.apply(lambda text: text.count(self.char))\n",
    "        return yelling_feature.values.reshape(-1, 1)\n",
    "\n",
    "class SwearingFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Converts a text feature into a binary feature where 1 denotes swearing occurs, which is usually\n",
    "    seen in text as a series of 2 or more asterisks, e.g. ****.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        BaseEstimator.__init__(self)\n",
    "        TransformerMixin.__init__(self)\n",
    "    \n",
    "    def fit(self, X: pd.Series, y: Optional[Any]=None) -> SwearingFeatureExtractor:\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.Series) -> pd.DataFrame:\n",
    "        yelling_feature = X.apply(lambda text: 0 + ('**' in text))\n",
    "        return yelling_feature.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb541186-4651-4eff-984a-b50ff39b673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> List[str]:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1295d9ef-c925-4bf8-af57-ef3f7cea91f4",
   "metadata": {},
   "source": [
    "## Reusable Funtionality to Evaluate a Model and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d85b4f7-d022-421a-b118-7cbdd7f0b388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_save_results(pipeline: Pipeline, model_num: int, train_on_selected_text: bool=False) -> None:\n",
    "\n",
    "    X_train_prepared = X_train.copy()\n",
    "    X_val_prepared = X_train.copy()\n",
    "    \n",
    "    if train_on_selected_text:\n",
    "        X_train_prepared['Text'] = X_train['Selected_Text']\n",
    "        X_val_prepared['Text'] = X_val['Selected_Text']\n",
    "    \n",
    "    pipeline.fit(X_train_prepared, y_train)\n",
    "    \n",
    "    y_val_pred = pipeline.predict(X_val_prepared)\n",
    "    y_test_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "    \n",
    "    test_data_exportable = test_data.copy()\n",
    "    test_data_exportable['Label'] = y_test_pred\n",
    "    test_data_exportable = test_data_exportable[['ID', 'Label']]\n",
    "    test_data_exportable.to_csv(f'outputs/model{model_num}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547bc283-8639-440b-ab4e-775b1d98685e",
   "metadata": {},
   "source": [
    "## A \"Switchboard\" to Enable and Disable Running Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ec2dc8f-f259-4423-bde7-49bbb10568ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_run = {\n",
    "    'model1': False,\n",
    "    'model2': False,\n",
    "    'model3': False,\n",
    "    'model4': False,\n",
    "    'model5': False,\n",
    "    'model6': False,\n",
    "    'model7': False,\n",
    "    'model8': False,\n",
    "    'model9': False,\n",
    "    'model10': False,\n",
    "    'model11': False,\n",
    "    'model12': False,\n",
    "    'model13': False,\n",
    "    'model14': True,\n",
    "    'model15': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f6f025-e9d6-44d4-91dd-719fd35a5599",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f71fec2b-ec70-4657-b292-ab6927e4917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_to_run['model1']:\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('text', TfidfVectorizer(), 'Text'),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', LogisticRegression(random_state=1)),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    evaluate_and_save_results(pipeline, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d26318e-fe4b-43e0-9f33-dc9197bf76e0",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a203be85-c41f-4b25-ab5e-0db3d651ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_to_run['model2']:\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('text', TfidfVectorizer(stop_words='english'), 'Text'),\n",
    "            ('numeric', StandardScaler(), ['User_Age', 'Time_of_Post', 'Population_Density']),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', RandomForestClassifier(n_estimators=200, random_state=1)),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    evaluate_and_save_results(pipeline, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb67b0d-8426-4782-a33d-8e3fd53ba086",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb3ac346-f71f-42a5-a04f-baf5558c84ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_to_run['model3']:\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('text', CountVectorizer(stop_words='english'), 'Text'),\n",
    "            ('numeric', StandardScaler(), ['User_Age', 'Time_of_Post', 'Population_Density']),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', RandomForestClassifier(n_estimators=200, random_state=1))\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    evaluate_and_save_results(pipeline, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c956ec00-a5ad-4c84-a46d-fce808dc1f12",
   "metadata": {},
   "source": [
    "## Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2690bd90-a9a6-488b-8f4f-9d2d1d27662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_to_run['model4']:\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('text', CountVectorizer(stop_words='english'), 'Text'),\n",
    "            ('numeric', MinMaxScaler(), ['User_Age', 'Time_of_Post', 'Population_Density']),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', MultinomialNB()),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    evaluate_and_save_results(pipeline, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dd400e-8e0f-4924-bf52-c435f9d3a334",
   "metadata": {},
   "source": [
    "## Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "366b87e8-9c6c-4937-a5c8-20e615d1f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_to_run['model5']:\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('text', CountVectorizer(stop_words='english'), 'Text'),\n",
    "            ('numeric', StandardScaler(), ['User_Age', 'Time_of_Post', 'Population_Density']),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', SVC(kernel='poly')),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    evaluate_and_save_results(pipeline, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e11bfd-094c-48eb-906f-a21502028c36",
   "metadata": {},
   "source": [
    "## Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2341d11b-4b3a-4982-abba-e62bfe4a040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_to_run['model6']:        \n",
    "    text_vectorizer = CountVectorizer(\n",
    "        stop_words='english',\n",
    "        # ngram_range=(1, 2)\n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('text', text_vectorizer, 'Text'),\n",
    "            ('numeric', StandardScaler(), ['User_Age', 'Time_of_Post', 'Population_Density']),\n",
    "            ('yelling', YellingBinaryFeatureExtractor(), 'Text'),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', RandomForestClassifier(n_estimators=200, random_state=1)),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    evaluate_and_save_results(pipeline, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab6e8e-eb7d-4f7b-bf78-ee5b2a350693",
   "metadata": {},
   "source": [
    "## Model 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e94c9d4-bc0b-4145-bb23-c10bade5ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_to_run['model7']:\n",
    "        \n",
    "    text_vectorizer = CountVectorizer(\n",
    "        stop_words='english',\n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('text', text_vectorizer, 'Text'),\n",
    "            ('capitalization', CapsCountFeatureExtractor(), 'Text'),\n",
    "            ('exclamation', ExclamationCountFeatureExtractor(), 'Text'),\n",
    "            ('swearing', SwearingFeatureExtractor(), 'Text'),\n",
    "            ('numeric', StandardScaler(), ['User_Age', 'Time_of_Post', 'Population_Density']),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', LogisticRegression(random_state=1, max_iter=1000)),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    evaluate_and_save_results(pipeline, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc77b1bf-8d1b-4351-b984-8d81aee2dcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_to_run['model8']:\n",
    "    text_vectorizer = CountVectorizer(\n",
    "        stop_words='english',\n",
    "        # ngram_range=(1, 2)\n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('text', text_vectorizer, 'Text'),\n",
    "            ('numeric', StandardScaler(), ['User_Age', 'Time_of_Post', 'Population_Density']),\n",
    "            ('capitalization', CapsCountFeatureExtractor(), 'Text'),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', RandomForestClassifier(n_estimators=200, random_state=1)),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    evaluate_and_save_results(pipeline, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e64f90b-c3b8-42b9-99fd-e9dfd873dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_to_run['model9']:\n",
    "    text_vectorizer = CountVectorizer(\n",
    "        stop_words='english',\n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('text', text_vectorizer, 'Text'),\n",
    "            ('numeric', StandardScaler(), ['User_Age', 'Time_of_Post', 'Population_Density']),\n",
    "            ('capitalization', CapsCountFeatureExtractor(), 'Text'),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', RandomForestClassifier(n_estimators=225, random_state=1)),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    evaluate_and_save_results(pipeline, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3c00bfa-076b-4766-b298-9cccecb2f102",
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_to_run['model10']:\n",
    "    text_vectorizer = CountVectorizer(\n",
    "        stop_words='english',\n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('text', text_vectorizer, 'Text'),\n",
    "            ('numeric', StandardScaler(), ['User_Age', 'Time_of_Post', 'Population_Density']),\n",
    "            ('capitalization', CapsCountFeatureExtractor(), 'Text'),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', RandomForestClassifier(n_estimators=225, random_state=1)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [220, 225, 230],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 10],\n",
    "        'min_samples_leaf': [1, 2],\n",
    "        'max_features': [None, 'sqrt', 'log2']\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(RandomForestClassifier(random_state=1), param_grid, cv=3, scoring='f1')\n",
    "    grid_search.fit(preprocessor.fit_transform(X_train_and_val), y_train_and_val)\n",
    "\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    # evaluate_and_save_results(pipeline, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adc3cd1c-b862-487d-9364-226f635d0097",
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_to_run['model11']:\n",
    "    text_vectorizer = CountVectorizer(\n",
    "        stop_words='english',\n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('text', text_vectorizer, 'Text'),\n",
    "            ('numeric', StandardScaler(), ['User_Age', 'Time_of_Post', 'Population_Density']),\n",
    "            ('capitalization', CapsCountFeatureExtractor(), 'Text'),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    voting_classifier = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('logistic_regression', LogisticRegression(random_state=1)),\n",
    "            ('random_forest', RandomForestClassifier(n_estimators=225, random_state=1)),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('voting', voting_classifier),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    evaluate_and_save_results(pipeline, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "122f7b47-9fcd-4cbc-88a4-7be457360430",
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_to_run['model12']:    \n",
    "    text_vectorizer = CountVectorizer(\n",
    "        stop_words='english',\n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('text', text_vectorizer, 'Text'),\n",
    "            ('numeric', StandardScaler(), ['User_Age', 'Time_of_Post', 'Population_Density']),\n",
    "            ('capitalization', CapsCountFeatureExtractor(), 'Text'),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', ThresholdClassifier(RandomForestClassifier(n_estimators=225, random_state=1), 0.55)),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    evaluate_and_save_results(pipeline, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9c7505e-6a9a-4fd6-a31a-60ca563c279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if models_to_run['model13']:\n",
    "    \n",
    "    text_vectorizer = CountVectorizer(\n",
    "        stop_words='english',\n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('text', text_vectorizer, 'Text'),\n",
    "            ('numeric', StandardScaler(), ['User_Age', 'Time_of_Post', 'Population_Density']),\n",
    "            ('capitalization', CapsCountFeatureExtractor(), 'Text'),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', RandomForestClassifier(n_estimators=221, random_state=1)),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    evaluate_and_save_results(pipeline, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9128c4cc-2eb8-4a2b-840e-6fa93f686275",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 22\u001b[0m\n\u001b[1;32m      7\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m ColumnTransformer(\n\u001b[1;32m      8\u001b[0m     transformers\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      9\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, text_vectorizer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     ]\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(\n\u001b[1;32m     16\u001b[0m     steps\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     17\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocessor),\n\u001b[1;32m     18\u001b[0m         (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m'\u001b[39m, LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)),\n\u001b[1;32m     19\u001b[0m     ]\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 22\u001b[0m \u001b[43mevaluate_and_save_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_on_selected_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36mevaluate_and_save_results\u001b[0;34m(pipeline, model_num, train_on_selected_text)\u001b[0m\n\u001b[1;32m      8\u001b[0m     X_val_prepared[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m X_val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSelected_Text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(X_train_prepared, y_train)\n\u001b[0;32m---> 12\u001b[0m y_val_pred \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_prepared\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m y_test_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_val, y_val_pred))\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/pipeline.py:514\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    512\u001b[0m Xt \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 514\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_params)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/utils/_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    163\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:827\u001b[0m, in \u001b[0;36mColumnTransformer.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;66;03m# ndarray was used for fitting or transforming, thus we only\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;66;03m# check that n_features_in_ is consistent\u001b[39;00m\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 827\u001b[0m Xs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_transform_one\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfitted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_as_strings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_dataframe_and_transform_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_output(Xs)\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Xs:\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;66;03m# All transformers are None\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:681\u001b[0m, in \u001b[0;36mColumnTransformer._fit_transform\u001b[0;34m(self, X, y, func, fitted, column_as_strings)\u001b[0m\n\u001b[1;32m    675\u001b[0m transformers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(\n\u001b[1;32m    677\u001b[0m         fitted\u001b[38;5;241m=\u001b[39mfitted, replace_strings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, column_as_strings\u001b[38;5;241m=\u001b[39mcolumn_as_strings\n\u001b[1;32m    678\u001b[0m     )\n\u001b[1;32m    679\u001b[0m )\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 681\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfitted\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m            \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mColumnTransformer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/pipeline.py:940\u001b[0m, in \u001b[0;36m_transform_one\u001b[0;34m(transformer, X, y, weight, **fit_params)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform_one\u001b[39m(transformer, X, y, weight, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params):\n\u001b[0;32m--> 940\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;66;03m# if we have a weight for this transformer, multiply output\u001b[39;00m\n\u001b[1;32m    942\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1434\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[0;32m-> 1434\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1436\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1275\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1277\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1278\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:105\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     doc \u001b[38;5;241m=\u001b[39m analyzer(doc)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:238\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    235\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_error)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan:\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m     )\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "if models_to_run['model14']:\n",
    "    \n",
    "    text_vectorizer = CountVectorizer(\n",
    "        stop_words='english',\n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('text', text_vectorizer, 'Text'),\n",
    "            #('numeric', StandardScaler(), ['User_Age', 'Time_of_Post', 'Population_Density']),\n",
    "            #('capitalization', CapsCountFeatureExtractor(), 'Text'),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', LogisticRegression(max_iter=1000, random_state=2)),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    evaluate_and_save_results(pipeline, 14, train_on_selected_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08040b78-ddfe-40aa-a273-a935062257da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
